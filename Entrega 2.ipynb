{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2qrgS_sCJquS"
   },
   "source": [
    "<img src = \"./resources/images/banner2.png\" alt = \"Encabezado MLDS\" width = \"100%\">  </img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a9669dfd"
   },
   "source": [
    "# **Definición e implementación de las tecnologías**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **0. Integrantes del equipo de trabajo**\n",
    "---\n",
    "\n",
    "<table><thead>\n",
    "  <tr>\n",
    "    <th>#</th>\n",
    "    <th>Integrante</th>\n",
    "    <th>Documento de identidad</th>\n",
    "  </tr></thead>\n",
    "<tbody>\n",
    "  <tr>\n",
    "    <td>1</td>\n",
    "    <td>Christian Enrique Córdoba Trillos</td>\n",
    "    <td>1030649666</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>2</td>\n",
    "    <td>Diego Alejandro Feliciano Ramos</td>\n",
    "    <td>1024586904</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>3</td>\n",
    "    <td>Ivonne Cristina Ruiz Páez</td>\n",
    "    <td>1014302058</td>\n",
    "  </tr>\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f6de2158"
   },
   "source": [
    "## **1. Tecnologías a utilizar**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "20031c7b"
   },
   "source": [
    "### **1.1. Selección de tecnologías**\n",
    "---\n",
    "* ¿Cuáles son las tecnologías seleccionadas para el proyecto?\n",
    "* ¿Por qué se eligieron estas tecnologías en lugar de otras alternativas?\n",
    "* ¿Cómo se alinean las tecnologías seleccionadas con los objetivos del proyecto?\n",
    "* ¿Cuáles son las ventajas clave de cada tecnología seleccionada?\n",
    "* ¿Existen desventajas o limitaciones que deben ser abordadas?\n",
    "---\n",
    "* **Tecnologías seleccionadas**:\n",
    "    * Modelo entrenado para clasificación de imágenes: Diseñado para identificar especies animales en imágenes provenientes de cámaras trampa, eliminando imágenes vacías o irrelevantes.\n",
    "    * PySpark (Spark para Python): Herramienta de procesamiento distribuido que permite manejar el preprocesamiento y la transformación de 2.5 millones de imágenes de manera eficiente.\n",
    "    * MongoDB: Base de datos NoSQL para almacenar metadatos en formato JSON, permitiendo consultas rápidas y flexibles, como filtrar por especie, fecha o ubicación.\n",
    "    * Servicios de almacenamiento en la nube (AWS S3, GCP Storage, Azure Blob Storage): Usados para gestionar de manera escalable el dataset masivo, con costos asumidos por los propietarios del dataset.\n",
    "    \n",
    "    Estas tecnologías fueron seleccionadas porque permiten resolver de forma óptima los retos del proyecto, como el manejo del volumen masivo de datos, la clasificación precisa y la accesibilidad global a los resultados.\n",
    "* **Razón de elección frente a otras alternativas**:\n",
    "    * Modelo de clasificación: Responde directamente a la necesidad de automatizar la identificación de especies en un dataset con 97 especies diferentes, abordando un desafío crítico para biólogos y conservacionistas.\n",
    "    * PySpark: Su arquitectura distribuida es fundamental para procesar el gran volumen de datos, algo que sería inviable con herramientas locales o no distribuidas.\n",
    "    * MongoDB: Es ideal para manejar datos semi-estructurados como los metadatos de las imágenes, superando las limitaciones de bases de datos relacionales que requieren esquemas rígidos.\n",
    "    * Servicios de almacenamiento en la nube: Ya están en uso por los propietarios del dataset, lo que elimina barreras económicas y técnicas para el equipo del proyecto, garantizando escalabilidad y confiabilidad.\n",
    "    \n",
    "    En conjunto, estas tecnologías se alinean con el objetivo del proyecto: clasificar imágenes de cámaras trampa y hacer uso eficiente de los recursos computacionales.\n",
    "* **Alineación de las tecnologías con los objetivos del proyecto**:\n",
    "    * Modelo de clasificación: Automatiza la tarea principal del proyecto, identificando especies animales y descartando imágenes irrelevantes.\n",
    "    * PySpark: Acelera el procesamiento masivo de imágenes, lo que reduce el tiempo necesario para analizar el dataset.\n",
    "    * MongoDB: Permite una gestión eficiente de los metadatos, facilitando análisis posteriores y optimizando la accesibilidad de la información clave.\n",
    "    * Servicios en la nube: Proveen la infraestructura necesaria para almacenar y acceder al dataset globalmente, asegurando que los resultados puedan ser utilizados por los beneficiarios del proyecto, como biólogos, ONGs y educadores.\n",
    "\n",
    "* **Ventajas clave de cada tecnología**:\n",
    "    * Modelo de clasificación: Automatiza tareas manuales intensivas y mejora la precisión en la clasificación de especies.\n",
    "    * PySpark: Procesa grandes datasets en paralelo, escalando eficientemente con la cantidad de datos.\n",
    "    * MongoDB: Soporte flexible para datos semi-estructurados, ideal para metadatos como especie, fecha y ubicación.\n",
    "    * Servicios en la nube: Ofrecen almacenamiento confiable y escalable, permitiendo la colaboración entre equipos distribuidos y el acceso a los datos en cualquier momento.\n",
    "    \n",
    "    Además, estas tecnologías contribuyen a resolver desafíos específicos, como el manejo de imágenes vacías o irrelevantes y la escalabilidad del procesamiento de datos.\n",
    "* **Posibles limitaciones/desventajas**:\n",
    "    * Modelo de clasificación: Puede enfrentar dificultades con imágenes de baja calidad o ruido excesivo.\n",
    "    * PySpark: Requiere habilidades técnicas avanzadas para configuraciones eficientes.\n",
    "    * MongoDB: Aunque flexible, su rendimiento puede degradarse si no se diseñan adecuadamente índices y consultas.\n",
    "    * Servicios en la nube: Aunque no hay costos directos para el equipo, la dependencia de proveedores externos puede ser un riesgo si cambian las políticas o los accesos.\n",
    "    \n",
    "    Estas limitaciones serán manejadas mediante una planificación cuidadosa, optimización de recursos y asegurando redundancia en los servicios utilizados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qG-gA0MvzMTA"
   },
   "source": [
    "## **2. Instalación de Herramientas**\n",
    "---\n",
    "- ¿Cuáles son las herramientas específicas que se instalarán en el entorno? ¿Por qué?\n",
    "- ¿Existen requisitos específicos del sistema para cada herramienta?\n",
    "- ¿Cómo se configurarán y personalizarán las herramientas para adaptarse a los requisitos específicos del proyecto?\n",
    "- ¿Existen configuraciones recomendadas para optimizar el rendimiento?\n",
    "---\n",
    "\n",
    "* **Herramientas a instalar y razón**:\n",
    "\n",
    "    * PySpark: Se utilizará para el procesamiento distribuido de los 2.5 millones de imágenes, permitiendo manejar grandes volúmenes de datos de forma eficiente. Esta herramienta es esencial para preprocesar imágenes, realizar transformaciones masivas y preparar los datos para el modelo de clasificación.\n",
    "    * Findspark: Facilita la integración de PySpark con entornos interactivos como Jupyter Notebook, mejorando la experiencia de desarrollo y depuración.\n",
    "\n",
    "* **Requisitos del sistema para cada herramienta**:\n",
    "\n",
    "    * PySpark:\n",
    "        * Python: Versión 3.8 o superior.\n",
    "        * Java JDK: Versión 8 o superior (necesario para Apache Spark).\n",
    "        * Memoria y CPU: \n",
    "            * Para entornos locales: \n",
    "                * Al menos 8 GB de RAM \n",
    "                * Procesadores multinúcleo para aprovechar el procesamiento paralelo.\n",
    "            * Para clústeres distribuidos: \n",
    "                * Un sistema operativo compatible (Linux/Ubuntu recomendado).\n",
    "                * Herramientas de orquestación como Hadoop o servicios en la nube compatibles con Spark (AWS EMR, Dataproc en GCP, etc.).\n",
    "                \n",
    "* **Pasos y scripts de instalación**:\n",
    "\n",
    "    * Spark\n",
    "\n",
    "        ```\n",
    "        pip install pyspark findspark\n",
    "        ```\n",
    "\n",
    "        * Verificar instalación:\n",
    "        \n",
    "            ```pyspark```\n",
    "\n",
    "        * Integrar PySpark con MongoDB: Descargar el conector MongoDB-Spark correspondiente y agregarlo como dependencia en Spark:\n",
    "        \n",
    "            ```wget https://repo1.maven.org/maven2/org/mongodb/spark/mongo-spark-connector_2.12/3.0.1/mongo-spark-connector_2.12-3.0.1.jar```                \n",
    "\n",
    "* **Configuración y personalización de las herramientas para los requisitos del proyecto**:\n",
    "\n",
    "    * Configuración de variables de entorno:\n",
    "        * Configurar rutas para Spark y Java:\n",
    "        \n",
    "            ```\n",
    "            export SPARK_HOME=/usr/local/spark\n",
    "            export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64\n",
    "            export PATH=$PATH:$SPARK_HOME/bin\n",
    "            ```\n",
    "        * Conexión con MongoDB: Usar el conector MongoDB-Spark para permitir la consulta y extracción de metadatos directamente desde MongoDB.\n",
    "\n",
    "            ```\n",
    "            pip install pymongo[srv]\n",
    "            pip install pyspark\n",
    "            ```\n",
    "\n",
    "        * Integración con almacenamiento en la nube:\n",
    "Configurar claves de acceso para AWS S3, GCP Storage y Azure Blob Storage dentro de PySpark usando bibliotecas específicas de almacenamiento.\n",
    "* **Configuraciones recomendadas**:\n",
    "\n",
    "    * Optimización del clúster Spark:\n",
    "        * Configurar correctamente la cantidad de executors(ejecutores), memoria y núcleos para maximizar el rendimiento:\n",
    "\n",
    "            ```\n",
    "            spark-submit --executor-memory 4G --total-executor-cores 8 app.py\n",
    "            ```\n",
    "\n",
    "        * Caché y partición: Usar la funcionalidad de caché de PySpark para datos que se reutilizan frecuentemente:\n",
    "\n",
    "            ```\n",
    "            df.cache()\n",
    "            ```\n",
    "\n",
    "        * Ajustar el número de particiones para equilibrar la carga de trabajo:\n",
    "\n",
    "            ```                \n",
    "            df.repartition(100)\n",
    "            ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2FLj-UiEd91C"
   },
   "source": [
    "## **Créditos**\n",
    "---\n",
    "\n",
    "* **Profesor:** [Jorge E. Camargo, PhD](https://dis.unal.edu.co/~jecamargom/)\n",
    "* **Asistentes docentes:**\n",
    "    - [Juan Sebastián Lara Ramírez](https://www.linkedin.com/in/juan-sebastian-lara-ramirez-43570a214/).\n",
    "* **Diseño de imágenes:**\n",
    "    - [Rosa Alejandra Superlano Esquibel](mailto:rsuperlano@unal.edu.co).\n",
    "* **Coordinador de virtualización:**\n",
    "    - [Edder Hernández Forero](https://www.linkedin.com/in/edder-hernandez-forero-28aa8b207/).\n",
    "    \n",
    "**Universidad Nacional de Colombia** - *Facultad de Ingeniería*"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
